{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidating the dataset\n",
    "\n",
    "## Step 1: Import the Data\n",
    "read in the CSV files\n",
    "\n",
    "Column titles: video_id,title,publishedAt,channelId,channelTitle,categoryId,trending_date,tags,view_count,likes,dislikes,comment_count,thumbnail_link,comments_disabled,ratings_disabled,description,trending_rank  \n",
    "\n",
    "\n",
    "## Step 2: Make Modifications\n",
    "Because I made changes as I went along, certain datasets need columns added to them and filled in. The data is already there, but it's easier to use in the later versions I pulled. I need to add an \"hour\" measure to my data, in addition to a \"current rank\" measure -- these are already present in the file after (DATE & HOUR).csv, but not prior -- though they are in the file's name.\n",
    "\n",
    "\n",
    "## Step 3: Concatenate the Files\n",
    "Now I need to combine the files into much bigger files, so I can actually analyze them.\n",
    "\n",
    "\n",
    "#note to self, also check the heiroglyphics lab for the method used to consolidate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#https://stackoverflow.com/questions/20906474/import-multiple-csv-files-into-pandas-and-concatenate-into-one-dataframe \n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "path = r'/home/WeiszL1/output' # use your path\n",
    "\n",
    "us_files = glob.glob(path + \"/*_US_videos.csv\")\n",
    "\n",
    "gb_files = glob.glob(path + \"/*_GB_videos.csv\")\n",
    "\n",
    "ca_files = glob.glob(path + \"/*_CA_videos.csv\")\n",
    "\n",
    "mx_files = glob.glob(path + \"/*_MX_videos.csv\")\n",
    "\n",
    "br_files = glob.glob(path + \"/*_BR_videos.csv\")\n",
    "\n",
    "de_files = glob.glob(path + \"/*_DE_videos.csv\")\n",
    "\n",
    "in_files = glob.glob(path + \"/*_IN_videos.csv\")\n",
    "\n",
    "jp_files = glob.glob(path + \"/*_JP_videos.csv\")\n",
    "\n",
    "kr_files = glob.glob(path + \"/*_KR_videos.csv\")\n",
    "\n",
    "#us_files = glob.glob(path + \"/*_US_videos.csv\")\n",
    "\n",
    "\n",
    "li = []\n",
    "\n",
    "'''\n",
    "Currently the vids are labelled 1-50 in trending rank, \n",
    "but it appears the actual list is much longer than this, which\n",
    "is resuting in duplicate rank values (repeating from 1-50 four times per csv) \n",
    "To remedy this, I'm adding a second column called \"full_rank\" that \n",
    "serves uses the index value of each row in a file as a ranking, from 1-200.\n",
    "THis \n",
    "My current theory is that each of these files IS ranked 1-200 in trending, \n",
    "but end users only see the top 50. Even if I'm wrong, though, this will still give us\n",
    "an accurate view of trending rank because the files list the current main trending videos \n",
    "first, always. So, we'll be able to see the main trending videos by looking at only those \n",
    "with a full_rank value in range 1:50. It also remedies the problem of this \"trending_rank\"\n",
    "value not being present in some early files.\n",
    "\n",
    "I also added the datetime values that I saved in the file names early on \n",
    "(bad data practice, I know) to the data table.\n",
    "\n",
    "'''\n",
    "#\\d*\\.\\d*\\.\\d*\\.\\d*\n",
    "for filename in us_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "                                            #\n",
    "    datetime = re.search(r'\\d*\\.\\d*\\.\\d*\\.\\d*', filename).group(0)  #find the date that I saved in the file name\n",
    "    df['trending_date'] = datetime                                  #make column \"trending_date\" and fill it with datetime values\n",
    "    \n",
    "    df['full_rank'] = np.arange(len(df)) + 1\n",
    "    \n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True, sort = False)\n",
    "\n",
    "\n",
    "frame.to_csv( \"combined_US_Data3.csv\", index=False, encoding='utf-8-sig')  #export as a csv\n",
    "\n",
    "###\n",
    "\n",
    "#\"19.23.10.17_US_\" = pd.read_csv('output/19.23.10.19_US_videos.csv',header=1,nrows=4,index_col=17)\n",
    "\n",
    "#copiersAdd = pd.read_csv('CopiersAdded.csv',header=2,nrows=2,index_col=0)\n",
    "#copiersService = pd.read_csv('CopiersService.csv',index_col=0)\n",
    "#print(copiers)\n",
    "#print(copiersAdd)\n",
    "#print(copiersService)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Export and gather non-US Countries' data.\n",
    "'''\n",
    "\n",
    "gb_files = glob.glob(path + \"/*_GB_videos.csv\")\n",
    "\n",
    "ca_files = glob.glob(path + \"/*_CA_videos.csv\")\n",
    "\n",
    "mx_files = glob.glob(path + \"/*_MX_videos.csv\")\n",
    "\n",
    "br_files = glob.glob(path + \"/*_BR_videos.csv\")\n",
    "\n",
    "de_files = glob.glob(path + \"/*_DE_videos.csv\")\n",
    "\n",
    "in_files = glob.glob(path + \"/*_IN_videos.csv\")\n",
    "\n",
    "jp_files = glob.glob(path + \"/*_JP_videos.csv\")\n",
    "\n",
    "kr_files = glob.glob(path + \"/*_KR_videos.csv\")\n",
    "\n",
    "#us_files = glob.glob(path + \"/*_US_videos.csv\")\n",
    "\n",
    "\n",
    "li = []\n",
    "\n",
    "\n",
    "\n",
    "for filename in gb_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "                                            #\n",
    "    datetime = re.search(r'\\d*\\.\\d*\\.\\d*\\.\\d*', filename).group(0)  #find the date that I saved in the file name\n",
    "    df['trending_date'] = datetime                                  #make column \"trending_date\" and fill it with datetime values\n",
    "    \n",
    "    df['full_rank'] = np.arange(len(df)) + 1\n",
    "    \n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True, sort = False)\n",
    "\n",
    "\n",
    "frame.to_csv( \"combined_GB_Data.csv\", index=False, encoding='utf-8-sig')  #export as a csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
